---
title: Faster-Rcnn翻译+详解
date: 2018-10-19 20:04:15
categories: 深度学习
tags:
	- 目标检测
comments: true
---

## Faster RCNN

我们将本文的目标检测系统称之为Faster Rcnn。其中包含两个部分，第一部分是深度全卷积网络模型来提取候选区域，第二个是Fast Rcnn识别模型。整个系统是统一的目标识别网络。如图所示，我们采用最近流行的术语“注意力”机制，使用RPN模型用来指引Fast Rcnn模型应该“看”哪里。在3.1中我们将介绍RPN网络的详细设计。在3.2中我们将设计算法使用特征共享的方式对两个模型训练。

![1552960089165](https://www.zkeenly.com/images/2019-03-19/1552960089165.png)

### 3.1 区域选择网络（RPN）

​	RPN采用任意大小的图像作为输入，输出一个目标矩阵框集合，每一个目标矩阵都包含对象的分数。我们采用全卷积网络完成处理，在这一章将详细描述。由于我们最终需要与Fast Rcnn 对比，我们采取与fast rcnn 使用一样的卷积层。我们采用ZF和VGG16测试。ZF包含5个可共享的卷积层，VGG16包含12个可共享的卷积层。

​	为了生成区域选择框，我们采用一个小的网络在最后输出的卷积特征图上滑动，这个小网络采用3*3的卷积层滑动。每一个滑动窗口都映射到题为特征中（ZF的256d，VGG16的512d，之后跟随Relu）。这个特征传送到两个同级全连接层（box-regression layer 以及 box-classification layer）。这个小网络如图3所示（左侧），

![1552960114278](https://www.zkeenly.com/images/2019-03-19/1552960114278.png)

#### 3.1.1 Anchors
在每一个滑动窗口的位置，我们同时预测多个区域候选，每个位置最多包含K个anchor boxes（通过每个位置扩展出不同形状的候选框），然后cls包含2k个输出结果，用来估计每个候选框是不是可能的目标对象。reg包含4k个输出结果用来保存每个候选框的坐标位置。ancher在滑动窗口的中心位置，并且由三个放大率和三个方向调节，每个滑动窗口中心点会生成9个ancher，这个输出的卷积特征图中一共输出大概2400个候选区域（每个候选区域都由两个scores 和四个coordinates）。

#####  平移不变Anchors

我们的方法一个最重要的属性是平移不变性，我们的方法保证了关于anchors 和计算相关区域到anchors的函数的平移不变性。平移不变性由全卷积网络的性质得到。相对的，在MultiBox方法中，使用k-means产生800个anchors ，是没有办法保证平移不变性的，所以MultiBox无法保证相同的区域在经过并以变换后仍然可以识别。

>  大概是说，卷积网络相对k-means更容易保持平移不变性

平移不变性同时降低了模型的大小，MultiBox由（4+1）* 800维度全连接输出层，然而我们的方法仅有（4+2）* 9维度的卷积输出层，在k=9anchors的时候。我们的输出层包含2.9 * 10^4个参数（VGG16的512* （4+2）* 9），而MultiBox‘s的输出包含6.1* 10^6参数（googlenet1536* （4+ 1）*  800）。如果考虑后面特征映射层，我们的方法相对MultiBox会少多个量级的参数，同时我们认为我们的方法将会具有更小的过拟合风险。

#####  Multi-Scale Anchors as Regression References

我们设计一个新的anchors的多尺度提取方法（多方向比例），如图所示，目前由两个主流的方法来进行多尺度预测。第一种方法是基于图像/特征金字塔（DPM，CNN方法）。图像将多尺度统一大小，然后经过特征提取（HOG,深度卷积），分别计算每一个尺度。这种方式往往时间开销很大。第二种方法是通过使用多适度的滑动窗口，不同尺度使用不同的滤波大小来训练（例如5* 7，7* 5）。如果使用这种方法解决多尺度问题，这可以称为滤波金字塔模型。通常两种方法联合采用。

![1552960150332](https://www.zkeenly.com/images/2019-03-19/1552960150332.png)

​	相应的，我们的anchor是用过建立一个更高效的“anchor金字塔”方法。我们的分类和多尺度回归边框的方法仅仅依赖于但尺度的图像和特征度，并且使用单尺度的滤波。我们采用这种有效地措施来解决多尺度和大小问题，由表中可以看出。

​	由于多尺度设计是基于anchors 的，我们可以完全使用卷积层来计算单尺度图像。这个多尺度anchors是一个解决多尺度问题而不增加更多开销的关键部分。

#### 3.1.2 损失函数

为了训练RPN，我们为每一个anchor赋值二分类标签。我们赋值正样本标签给两种anchors：1，这个anchor/cnchors 是一个最高的intersection-over-union（iou），基准box重叠率。2，一个anchor的重叠率高于0.7其他基准box。每个基准box可能会赋值给多个anchors正标签。通常第二种是足够决定正样本了；但是我们仍然采用第一种情况，由于在少量情况第二种也许不能找到正样本。如果Iou率是少于0.3的，我们为所有基准box的非正anchor赋值负标签。如果anchors 不是正的也不是负的，那对训练目标没有帮助。

我们在训练Fast Rcnn最小化目标函数通过多任务损失函数。我们图像的损失函数定义为：

![1552960224187](https://www.zkeenly.com/images/2019-03-19/1552960224187.png)

- i 是这个anchor在mini-batch 的索引，
- p<sub>i</sub>是anchor i 是一个目标的概率。
- 如果anchor是正样本，基准标签p<sub>i</sub><sup>* </sup>是1。负样本则是0。
- t<sub>i</sub>是一个向量，代表4个坐标的参数。来预测bounding box。
- t<sub>i</sub><sup> * </sup> 是相应的基准正样本标签。
- 类别损失L<sub>cls</sub> 是两个类别的log损失函数。
- 回归损失我们使用L<sub>reg</sub> (t<sub>i</sub>, t<sub>i</sub> <sup>* </sup> ) = R(t<sub>i</sub> -  t<sub>i</sub><sup> *  </sup>  )
- R是[2] 中定义的robust损失函数（smooth L<sub>1</sub>）。
- p<sub>i</sub> <sup>* </sup> L<sub>reg</sub> 意味着回归损失仅仅当正样本（p<sub>i</sub> <sup>* </sup> =1）的时候被激活，放负样本（p<sub>i</sub> <sup>* </sup> =0）的时候激活。
- 输出层cls 和reg 分别包含{p<sub>i</sub> }和{t<sub>i</sub>}。

这两部分由N<sub>cls</sub> 和N<sub>reg</sub> 以及平衡参数的权重 \lambda 标准化。在我们当前的实现方法中，cls列表在公式（1）中是被mini-batch size标准化的。reg 由anchor 位置的数量标准化的。我们默认设置 \lambda =10，并且使cls 和reg 具有大概相等的权重。我们通过实现设置不同的lambda 值得到不同的结果。所以标准化是不需要过于强调，而应该尽量简化。

![1552987177356](https://www.zkeenly.com/images/2019-03-19/1552987177356.png)

​	对于边框回归，我们采用四个坐标的参数化:

![1552987990659](https://www.zkeenly.com/images/2019-03-19/1552987990659.png)

其中，x,y,w以及h决定这个box的中心坐标以及宽和高。变量 x, x<sub>a</sub> ,以及x<sup>* </sup> 分别是预测box， anchorbox，以及基准box。这样可以使anchor box 通过bounding-box回归到附近的基准box。

​	此外，我们的方法bounding-box 回归方法不同于其他Roi等方法，bounding-box 回归可以得到任意的Roi大小，回归权重可以与所有大小的范围共享。在我们的公式中，用于回归的特征提取在特征图上使用了相同的卷积大小（3* 3）。为了得到不同的大小，我们设置学习k个回归框。每一个回归是代表一个尺度和方向改变率，并且k回归不能共享权重。同样的，得益于anchors 的设计，它也是可以通过修复尺度和大小来预测不同大小的。

### 3.1.3 训练 RPNS

​	RPN可以通过反向传播以及梯度下降训练端到端网络。我们“image-centric”策略来训练网络。每次从单一图像包含正负样例的anchors mini-batch 。为所有anchor损失函数优化是可能的，但是这将会偏向于负样本，因为负样本是多数的。相反，我们随机256个anchors 样本在图像中计算mini-batch损失函数，在这里正负样本比例为1：1。如果少于128个正样本，那么我们使用负样本填充mini-batch。

​	我们通过高斯标准分布0.01，随机初始化所有层的权重。其他（共享权重的）使用预处理模型初始化来作为标准。我们使用学习率为0.001 在60K个minibatchs上，学习率为0.0001在生于20K个mini-batchs上，我们采用动量优化参数为0.9，权重衰退为0.0005，采用caffe实现。

## 3.2 RPN与Fast Rcnn的特征共享

现在，我们将要描述如何训练区域范围生成网络，不考虑利用这些范围的基于区域的目标识别CNN。对于检测网络，我们采用 Fast Rcnn。接下来我们描述将RPN和FastRcnn一体化的算法，并且共享卷积层，如图2.

将RPN和Fast Rcnn 训练单独训练将会以不同的方式修改卷积层。我们因此需要一种技术来允许共享共享两个网络的卷积层。我们以三种特征共享的方式训练网络：

（i）Alternating training，这种解决方法，我们首先训练RPN，并且使用目标区域来训练 Fast Rcnn，这个网络通过Fast Rcnn 来微调经过初始化过的RPN，并且这个方法是可迭代的。这是在本文所有实验中采用的一种方法。

（ii）Approximate joint training（近似联合训练）. 这种方法，RPN和Fast Rcnn 网络被融合在一个网络中，像图2中训练的。在每次SGD迭代中，正向传播中的区域生成参数被固定，当训练Fast Rcnn检测器的时候预先计算区域建议框。反向传播和往常相同，共享层的反向传播信号中RPN和Fast Rcnn的loss相结合。这个方案是容易实现的。但是这个方法忽视了衍生的候选盒子坐标 w.r.t.，这个坐标也是网络的回应，所以是近似的。在我们的实验中，我们的经验中发现这个解决方案产生了相近的结果，但是降低了25%的计算时间。这个方案在哦我们提供的Python 代码中。

（iii）Non-approximate joint training. 根据以上讨论，这个通过RPN的边框预测也是一个输入函数。在Fast Rcnn的RoI 池化层 接受这个卷积特征并且预测边框作为一个输入，因此，一个理论上有效的反向传播求解器也应该包含梯度w.r.t。上述近似联合训练忽略了这些梯度。在非近似联合训练解中，我们需要一个RoI池层，它是可导的w.r.t.坐标。这是一个非常重要的问题，可以通过[15]中开发的“RoI池化”层给出解决方案，这超出了本文的讨论范围。

**四步交替训练** 。在这个文中，我们采用实用的四步训练算法来通过交替优化学习共享特征。在第一步，我们如3.1.3训练RPN。这个网络由ImageNet 预训练模型初始化，并且微调这个端到端的区域选择任务。在第二步骤，我们通过Fast Rcnn使用第一步骤区域生成的RPN，训练一个可分离的检测网络。这个检测网络也有ImageNet预训练模型来训练。在这一点，两个网络不能够共享卷积层。第三步骤，我们使用检测网络初始化RPN训练模型，然后固定共享的卷积层，并且只微调RPN层。现在这两个网络共享了卷积层。最终保持这个共享卷积层固定，然后微调FastRcnn的网络层。如此，两个网络合并为统一网络并共享相同的卷积层。一个相似的交替训练方法可以运行多次，但是我们看到之后的改善并不明显。

## 3.3 实现细节

我们训练和测试 单尺度图像上的区域选择和对象检测网络[1] [2]。我们统一调整尺寸为短边600 像素[2]。多尺度特征提取（使用image pyramid）也许会改善精度但是不能有一个好的速度以及准确速度的权衡[2]。在重新调整图像尺度，在最后一个卷积层上，ZF 和VGG网络总步幅都是16像素。并且因此，在调整大小之前，典型的pascal网络具有大概10个像素。尽管如此，大尺度的步幅仍会有一个好的结果，尽管小的尺度可能会有更好的精度。

对于anchors，我们采用3个尺度，128<sup>2</sup> ,256<sup>2</sup> , 512<sup>2</sup> 。以及三个方向调整 1：1，1：2，2：1.这些这些超参数没有根据特别的数据集进行选择，我们下一章有具体实验结果。在我们的讨论中，我们的方法不需要图像金字塔或者滤波器金字塔来预测多个尺度的区域，从而节省了运行时间。图3右侧显示了我们对于多个尺度的预测效果。表1显示了学习到的每一个anchor平均区域大小。我们的算法允许比潜在感知区域更大范围的预测。这样的预测并不是不可能的-如果一个物体的中心是可见的，那么可以大概的估计出物体的范围。

![1553472595147](https://www.zkeenly.com/images/2019-03-19/1553472595147.png)

穿过图像边界的anchor boxes需要谨慎处理，在训练的时候，我们忽视所有越过边界的anchors，它们对loss没有帮助。对于典型的1000* 600图像，一共大概会有20000个（60* 40 * 9）个anchor 。随着边界anchors被忽略，大概每张图片一共有6000个anchors用于训练，如果这个越过边界的anchors 不被忽略，则会在目标函数中引入大的，难以纠正的误差项，且训练不会收敛。但在测试过程中，我们仍然将全卷积RPN应用于整张图像。这可能会产生跨边界的提议边界框，我们剪切到图像边界。

​	一些RPN提议互相之间高度重叠。为了减少冗余，我们在提议区域根据他们的*cls*分数采取非极大值抑制（NMS）。我们将NMS的IoU阈值固定为0.7，这就给每张图像留下了大约2000个提议区域。正如我们将要展示的那样，NMS不会损害最终的检测准确性，但会大大减少提议的数量。在NMS之后，我们使用前N个提议区域来进行检测。接下来，我们使用2000个RPN提议对Fast R-CNN进行训练，但在测试时评估不同数量的提议。



## 总结

简单的说，整个流程为

![1553479914604](https://www.zkeenly.com/images/2019-03-19/1553479914604.png)

input-特征提取-RPN生成候选区域-RoI统一大小-Fast Rcnn预测目标分类

1. 假设原图为720*1280，resize为600 * 1067

1. input图像经过conv5_3 特征提取生成1 * 38 * 67 * 1024。的特征图。

2. 将38 * 67 的特征图传送到RPN中，进行预测是前景还是背景。
   1. RPN网络具体为采用3* 3的滤波器以stride=1 滑动特征图。每次滑动为一个anchor，然后这个anchor扩展为9个不同方向和大小的anchor。一共得到38 * 67 * 9个原始图片proposal。
   2. 将剔除越过边界的anchor后剩余部分的anchor 进行预测是前景还是背景，输出cls_score（分类参数），以及bbox_pred（坐标参数）。
   3. 将得到的结果进行非极大抑制，大概留下2000个框作为RPN。
3. 将2000个RPN与特征图相结合，进行RoI Pooling，得到固定尺寸的特征图。送至Fast Rcnn中进行预测。













