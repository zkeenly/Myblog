---
title: Faster-Rcnn翻译+详解
date: 2018-10-19 20:04:15
categories: 深度学习
tags:
	- 目标检测
comments: true
---

Faster RCNN

我们的目标检测系统，称之为Faster Rcnn，包含两个部分。第一部分是深度全卷积网络模型来提供区域范围，第二个是Fast Rcnn识别模型。整个系统是单个统一的对象识别网络如图所示。使用最近流行的术语“注意力”机制，RPN模型用来告诉Fast Rcnn模型应该看哪里。在3.1中我们将介绍RPN网络的设计和内容。在3.2中我们将设计算法将RPN特征与Fast Rcnn共享。

![1552960089165](https://www.zkeenly.com/images/2019-03-19/1552960089165.png)

3.1 区域选择网络（RPN）

​	RPN采用任何大小的图像作为输入，输出一个目标矩阵框集合，以及每一个对象的分数。我们采用全卷积网络完成处理，在这一章将详细描述。由于我们最终需要与Fast Rcnn 对比，我们采取与fast rcnn 使用一样的卷积层。我们采用ZF和VGG16测试。ZF包含5个可共享的卷积层，VGG16包含12个可共享的卷积层。

为了生成区域选择框，我们采用一个小的网络在最后输出的卷积特征图上滑动，这个小网络采用3*3的卷积层滑动。每一个滑动窗口都映射到题为特征中（ZF的256d，VGG16的512d，之后跟随Relu）。这个特征传送到两个同级全连接层（box-regression layer 以及 box-classification layer）。这个小网络如图3所示（左侧），

![1552960114278](https://www.zkeenly.com/images/2019-03-19/1552960114278.png)

在每一个滑动窗口的位置，我们同时预测多个区域候选，每个位置最多包含K个anchor boxes（通过每个位置扩展出不同形状的候选框），然后cls包含2k个输出结果，用来估计每个候选框是不是可能的目标对象。reg包含4k个输出结果用来保存每个候选框的坐标位置。ancher在滑动窗口的中心位置，并且由三个放大率和三个方向调节，每个滑动窗口中心点会生成9个ancher，这个输出的卷积特征图中一共输出大概2400个候选区域（每个候选区域都由两个scores 和四个coordinates）。

平移不变Anchors

我们的方法一个最重要的属性是平移不变性，我们的方法保证了关于anchors 和计算相关区域到anchors的函数的平移不变性。平移不变性由全卷积网络的性质得到。相对的，在MultiBox方法中，使用k-means产生800个anchors ，是没有办法保证平移不变性的，所以MultiBox无法保证相同的区域在经过并以变换后仍然可以识别。

`大概是说，卷积网络相对k-means更容易保持平移不变性`

平移不变性同时降低了模型的大小，MultiBox由（4+1）* 800维度全连接输出层，然而我们的方法仅有（4+2）* 9维度的卷积输出层，在k=9anchors的时候。我们的输出层包含2.9 * 10^4个参数（VGG16的512* （4+2）* 9），而MultiBox‘s的输出包含6.1* 10^6参数（googlenet1536* （4+ 1）*  800）。如果考虑后面特征映射层，我们的方法相对MultiBox会少多个量级的参数，同时我们认为我们的方法将会具有更小的过拟合风险。

Multi-Scale Anchors as Regression References

我们设计一个新的anchors的多尺度提取方法（多方向比例），如图所示，目前由两个主流的方法来进行多尺度预测。第一种方法是基于图像/特征金字塔（DPM，CNN方法）。图像将多尺度统一大小，然后经过特征提取（HOG,深度卷积），分别计算每一个尺度。这种方式往往时间开销很大。第二种方法是通过使用多适度的滑动窗口，不同尺度使用不同的滤波大小来训练（例如5* 7，7* 5）。如果使用这种方法解决多尺度问题，这可以称为滤波金字塔模型。通常两种方法联合采用。

![1552960150332](https://www.zkeenly.com/images/2019-03-19/1552960150332.png)

​	相应的，我们的anchor是用过建立一个更高效的“anchor金字塔”方法。我们的分类和多尺度回归边框的方法仅仅依赖于但尺度的图像和特征度，并且使用单尺度的滤波。我们采用这种有效地措施来解决多尺度和大小问题，由表中可以看出。

​	由于多尺度设计是基于anchors 的，我们可以完全使用卷积层来计算单尺度图像。这个多尺度anchors是一个解决多尺度问题而不增加更多开销的关键部分。

损失函数

为了训练RPN，我们为每一个anchor赋值二分类标签。我们赋值正样本标签给两种anchors：1，这个anchor/cnchors 是一个最高的intersection-over-union（iou），基准box重叠率。2，一个anchor的重叠率高于0.7其他基准box。每个基准box可能会赋值给多个anchors正标签。通常第二种是足够决定正样本了；但是我们仍然采用第一种情况，由于在少量情况第二种也许不能找到正样本。如果Iou率是少于0.3的，我们为所有基准box的非正anchor赋值负标签。如果anchors 不是正的也不是负的，那对训练目标没有帮助。

我们在训练Fast Rcnn最小化目标函数通过多任务损失函数。我们图像的损失函数定义为：

![1552960224187](https://www.zkeenly.com/images/2019-03-19/1552960224187.png)

- i 是这个anchor在mini-batch 的索引，
- p<sub>i</sub>是anchor i 是一个目标的概率。
- 如果anchor是正样本，基准标签p<sub>i</sub><sup>* </sup>是1。负样本则是0。
- t<sub>i</sub>是一个向量，代表4个坐标的参数。来预测bounding box。
- t<sub>i</sub><sup> * </sup> 是相应的基准正样本标签。
- 类别损失L<sub>cls</sub> 是两个类别的log损失函数。
- 回归损失我们使用L<sub>reg</sub> (t<sub>i</sub>, t<sub>i</sub> <sup>* </sup> ) = R(t<sub>i</sub> -  t<sub>i</sub><sup> *  </sup>  )
- R是[2] 中定义的robust损失函数（smooth L<sub>1</sub>）。
- p<sub>i</sub> <sup>* </sup> L<sub>reg</sub> 意味着回归损失仅仅当正样本（p<sub>i</sub> <sup>* </sup> =1）的时候被激活，放负样本（p<sub>i</sub> <sup>* </sup> =0）的时候激活。
- 输出层cls 和reg 分别包含{p<sub>i</sub> }和{t<sub>i</sub>}。

这两部分由N<sub>cls</sub> 和N<sub>reg</sub> 以及平衡参数的权重 \lambda 标准化。在我们当前的实现方法中，cls列表在公式（1）中是被mini-batch size标准化的。reg 由anchor 位置的数量标准化的。我们默认设置 \lambda =10，并且使cls 和reg 具有大概相等的权重。我们通过实现设置不同的lambda 值得到不同的结果。所以标准化是不需要过于强调，而应该尽量简化。

![1552987177356](https://www.zkeenly.com/images/2019-03-19/1552987177356.png)

​	对于边框回归，我们采用四个坐标的参数化:

![1552987990659](https://www.zkeenly.com/images/2019-03-19/1552987990659.png)

其中，x,y,w以及h决定这个box的中心坐标以及宽和高。变量 x, x<sub>a</sub> ,以及x<sup>* </sup> 分别是预测box， anchorbox，以及基准box。这样可以使anchor box 通过bounding-box回归到附近的基准box。

​	此外，我们的方法bounding-box 回归方法不同于其他Roi等方法，bounding-box 回归可以得到任意的Roi大小，回归权重可以与所有大小的范围共享。在我们的公式中，用于回归的特征提取在特征图上使用了相同的卷积大小（3* 3）。为了得到不同的大小，我们设置学习k个回归框。每一个回归是代表一个尺度和方向改变率，并且k回归不能共享权重。同样的，得益于anchors 的设计，他也是可以通过修复尺度和大小来预测不同大小的。

训练 RPNS

​	RPN可以通过反向传播以及梯度下降训练端到端网络。我们“image-centric”策略来训练网络。每次从单一图像包含正负样例的anchors mini-batch 。为所有anchor损失函数优化是可能的，但是这将会偏向于负样本，因为负样本是多数的。相反，我们随机256个anchors 样本在图像中计算mini-batch损失函数，在这里正负样本比例为1：1。如果少于128个正样本，那么我们使用负样本填充mini-batch。

​	我们通过高斯标准分布0.01，随机初始化所有层的权重。其他（共享权重的）使用预处理模型初始化来作为标准。我们使用学习率为0.001 在60K个minibatchs上，学习率为0.0001在生于20K个mini-batchs上，我们采用动量优化参数为0.9，权重衰退为0.0005，采用caffe实现。



























